data_ingestion:
  bucket_name: "sontha_bucket"
  zip_file_name: "hate_speech_dataset.zip"
  artifacts_dir: "artifacts"
  ingestion_dir: "data_ingestion"
  imbalance_file_name: "dataset.csv"  
  raw_file_name: "dataset.csv" 
         
preprocessing:
  cleaned_file_name: "cleaned_data.csv"
  stopwords: "english"

embeddings:
  artifacts_dir: "artifacts"
  max_words: 10000
  max_seq_length: 100
  embedding_dim: 100
  glove_file: "glove.6B.100d.txt"
  embedded_matrix_file: "embedding_matrix.npy"
  tokenizer_file: "tokenizer.pkl"

model:
  model_architecture_file: "artifacts/model/model_architecture.json"
  conv1d_filters: 128
  kernel_size: 5
  pool_size: 2
  lstm_units: 64
  lstm_dropout_rate: 0.5
  lstm_recurrent_dropout_rate: 0.3
  dropout_rate: 0.4
  dense_units: 64
  dense_dropout_rate: 0.3
  l2_regularization: 0.01
  activation: "relu"
  final_activation: "sigmoid"

training:
  loss: "binary_crossentropy"
  optimizer: "adam"
  metrics: ["accuracy"]
  model_save_path: "artifacts/model/model.h5"
  epochs: 20
  batch_size: 32
  validation_split: 0.2
  patience: 5
  class_threshold: 0.4

evaluation:
  metrics_file: "artifacts/model/evaluation_metrics.json"
  best_accuracy_file: "artifacts/model/best_f1_hate.txt"

pusher:
  bucket_name: "sontha_bucket"
  model_name: "best_model.h5"